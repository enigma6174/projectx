{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8abecc-e9eb-4579-9bcf-f095afe944c5",
   "metadata": {},
   "source": [
    "## Step 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69098282-78fc-4be0-9093-625d3ad68768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# Filename\n",
    "filename = \"data/llama2.pdf\"\n",
    "\n",
    "# Partition the document into raw elements\n",
    "elements = partition(filename=filename, strategy=\"fast\")\n",
    "\n",
    "# Chunk the elements of the partitioned document\n",
    "chunks = chunk_by_title(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aeb8d1-6845-45b5-95a0-f428de7aecbf",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Vector Store And Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737096c2-1e38-4bc6-ae78-f72fe70f42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize documents array\n",
    "documents = []\n",
    "\n",
    "# Process each chunk and remove the language key from metadata\n",
    "for chunk in chunks:\n",
    "    metadata = chunk.metadata.to_dict()\n",
    "    metadata.pop('languages')\n",
    "    documents.append(Document(page_content=chunk.text, metadata=metadata))\n",
    "\n",
    "# Initialize the vector db\n",
    "db = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    collection_name=\"chroma-db-papers\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./db\"\n",
    ")\n",
    "\n",
    "# Create a retriever over the vector database\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 6, \"lambda_mult\": 0.25, \"fetch_k\": 30}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bbb35-ca68-4d0d-816f-29c8f58a0848",
   "metadata": {},
   "source": [
    "## Step 3: Create The Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d124ae19-c3bb-4d6c-b225-cd13b8f22fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Pull a prompt template\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9acb2bbf-5aef-462b-bf1d-4a11cf996a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke(\n",
    "    \"\"\"What is reward modelling in llama2?\n",
    "    Provide citations or references at the end.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3400e497-0adc-4743-8cad-188abad34507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reward modeling in Llama 2 involves taking a model response and its corresponding prompt as inputs and outputting a scalar score to indicate the quality of the response, such as its helpfulness and safety. These scores are used as rewards to optimize Llama 2-Chat during RLHF (Reinforcement Learning with Human Feedback) for better alignment with human preferences. This process aims to enhance the model's helpfulness and safety (3.2.2 Reward Modeling).\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b4c6c-8015-4cd2-a916-66d96f9b8539",
   "metadata": {},
   "source": [
    "## Optional: Using A Pre-Existing Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cee8d038-7fa1-4331-a89f-5f44d180d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLECTION_NAME: chroma-db-manuals\n",
      "COLLECTION_METADATA: None\n",
      "\n",
      "COLLECTION_NAME: chroma-db-papers\n",
      "COLLECTION_METADATA: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Pull a prompt template\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create a chromadb client\n",
    "try:\n",
    "    client = chromadb.PersistentClient(\"./db\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# View existing collections\n",
    "collections = client.list_collections()\n",
    "for collection in collections:\n",
    "    print(f\"COLLECTION_NAME: {collection.name}\\nCOLLECTION_METADATA: {collection.metadata}\\n\")\n",
    "\n",
    "# Load the required db file and collection name\n",
    "db = Chroma(\n",
    "    persist_directory=\"./db\", \n",
    "    collection_name=\"chroma-db-manuals\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create a retriever over the database\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 6, \"lambda_mult\": 0.25, \"fetch_k\": 50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3da895f-2084-474c-ba9d-a09f711043cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversation chain\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "247572cd-fe9a-478f-b40c-24ee1851f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke(\n",
    "    \"\"\"How to select a new rhythm pattern in RC 10R?\n",
    "    Provide citations or references at the end.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f2cb6dd-4edb-49b1-a362-5d527f5519a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To select a new rhythm pattern in the RC-10R, first press the [VALUE] knob to move the cursor to the genre. Then, turn the [VALUE] knob to select the desired genre. If the screen is not correct, press the [EXIT] button several times to access the top screen.\\n\\nReferences: BOSS RC-10R Basic Rhythm Operation.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339258e6-cf0c-4633-b92d-878969aa8f71",
   "metadata": {},
   "source": [
    "## Step 3: Initializing The Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecee44e-85d3-495e-a569-2d3445c4a81f",
   "metadata": {},
   "source": [
    "### Using In-Memory Vector Store (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd2ce6-1816-4791-9631-35a4d483e683",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Create a document from every chunk and add to document store\n",
    "for chunk in chunks:\n",
    "    metadata = chunk.metadata.to_dict()\n",
    "    documents.append(Document(page_content=chunk.text, metadata=metadata))\n",
    "\n",
    "# Initialize in-memory vector index with the document store\n",
    "db = FAISS.from_documents(\n",
    "    documents, \n",
    "    HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-base-en-v1.5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create retriever to index over the document store\n",
    "faiss_retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32718aff-28f5-4601-ac33-d9921f9c4236",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "retrieved_documents = retriever.invoke(\"llama2 pretrained model evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145be59c-ab29-4dd3-9256-d42150cbfb0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for document in retrieved_documents:\n",
    "    print(f\"CONTENT:\\n{document.page_content}\\n\\nMETADATA:\\npage: {document.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d6f4b-b241-44fa-97a3-8893cca768d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
